{"cells":[{"cell_type":"markdown","metadata":{},"source":["# CapsNet MNIST Experiment"]},{"cell_type":"markdown","metadata":{},"source":["## Envoirnment Setting "]},{"cell_type":"markdown","metadata":{},"source":["Import the Require Libraries"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T05:30:00.660555Z","iopub.status.busy":"2024-07-07T05:30:00.660083Z","iopub.status.idle":"2024-07-07T05:30:00.666421Z","shell.execute_reply":"2024-07-07T05:30:00.665499Z","shell.execute_reply.started":"2024-07-07T05:30:00.660501Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.optim import Adam\n","from torchvision import datasets, transforms\n","\n","USE_CUDA = True"]},{"cell_type":"markdown","metadata":{},"source":["## The Architecture of CapsNet"]},{"cell_type":"markdown","metadata":{},"source":["A block of Convolution layer and Relu Activation on it"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T05:30:00.668747Z","iopub.status.busy":"2024-07-07T05:30:00.668434Z","iopub.status.idle":"2024-07-07T05:30:00.675922Z","shell.execute_reply":"2024-07-07T05:30:00.675122Z","shell.execute_reply.started":"2024-07-07T05:30:00.668722Z"},"trusted":true},"outputs":[],"source":["class ConvLayer(nn.Module):\n","    def __init__(self, in_channels=1, out_channels=256, kernel_size=9):\n","        super(ConvLayer, self).__init__()\n","\n","        self.conv = nn.Conv2d(in_channels=in_channels,\n","                               out_channels=out_channels,\n","                               kernel_size=kernel_size,\n","                               stride=1\n","                             )\n","\n","    def forward(self, x):\n","        return F.relu(self.conv(x))"]},{"cell_type":"markdown","metadata":{},"source":["**Reason:** This class creates the first layer of capsules directly from the input data. It transforms the convolutional output into capsule vectors, preparing them for the next layers."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T05:30:00.677230Z","iopub.status.busy":"2024-07-07T05:30:00.676893Z","iopub.status.idle":"2024-07-07T05:30:00.687857Z","shell.execute_reply":"2024-07-07T05:30:00.686878Z","shell.execute_reply.started":"2024-07-07T05:30:00.677206Z"},"trusted":true},"outputs":[],"source":["\n","class PrimaryCaps(nn.Module):\n","    def __init__(self, num_capsules=8, in_channels=256, out_channels=32, kernel_size=9):\n","        super(PrimaryCaps, self).__init__()\n","\n","        # Create a list of convolutional layers (capsules)\n","        # Each capsule applies a convolution to the input image\n","        # `in_channels`: Number of input channels (features)\n","        # `out_channels`: Number of output channels (features) for each capsule\n","        # `kernel_size`: Size of the convolutional filter\n","        # `stride`: Step size for moving the filter across the image\n","        self.capsules = nn.ModuleList([\n","            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=2, padding=0) \n","                          for _ in range(num_capsules)])\n","    \n","    def forward(self, x):\n","        # Apply each capsule (convolutional layer) to the input\n","        # `u` is a list of outputs from each capsule\n","        u = [capsule(x) for capsule in self.capsules]\n","        \n","        # Stack the outputs along a new dimension to group them together\n","        # `u` dimensions: (batch_size, num_capsules, out_channels, height, width)\n","        u = torch.stack(u, dim=1)\n","        \n","        # Reshape `u` to combine all spatial dimensions and capsules\n","        # `u` dimensions: (batch_size, num_routes, out_channels)\n","        # `num_routes` is the total number of features across all capsules\n","        u = u.view(x.size(0), 32 * 6 * 6, -1)\n","        \n","        # Apply the squash function to the output\n","        return self.squash(u)\n","    \n","    def squash(self, input_tensor):\n","        # Squash function to ensure the output vectors have lengths between 0 and 1\n","        # Calculate the squared norm of the input tensor along the last dimension\n","        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n","        \n","        # Adjust the length of the input tensor using the squared norm\n","        output_tensor = squared_norm * input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm + 1e-8))\n","        return output_tensor\n"]},{"cell_type":"markdown","metadata":{},"source":["**Reason:** This class represents the digit capsules, responsible for high-level feature detection. It performs the dynamic routing-by-agreement algorithm, which iteratively updates the coupling coefficients (weights) to determine the contribution of each capsule from the previous layer to each capsule in the current layer."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T05:30:00.689151Z","iopub.status.busy":"2024-07-07T05:30:00.688895Z","iopub.status.idle":"2024-07-07T05:30:00.700511Z","shell.execute_reply":"2024-07-07T05:30:00.699629Z","shell.execute_reply.started":"2024-07-07T05:30:00.689129Z"},"trusted":true},"outputs":[],"source":["class DigitCaps(nn.Module):\n","    def __init__(self, num_capsules=10, num_routes=32 * 6 * 6, in_channels=8, out_channels=16):\n","        super(DigitCaps, self).__init__()\n","\n","        # Number of features going into each capsule\n","        self.in_channels = in_channels\n","        # Number of connections (routes) from previous layer to capsules in this layer\n","        self.num_routes = num_routes\n","        # Number of capsules in this layer\n","        self.num_capsules = num_capsules\n","\n","        # Initialize a weight matrix for transforming input data\n","        # It has dimensions: (1, num_routes, num_capsules, out_channels, in_channels)\n","        # 1: to allow broadcasting across batches\n","        # num_routes: number of input connections per capsule\n","        # num_capsules: number of capsules in this layer\n","        # out_channels: number of output features per capsule\n","        # in_channels: number of input features per capsule\n","        self.W = nn.Parameter(torch.randn(1, num_routes, num_capsules, out_channels, in_channels))\n","\n","    def forward(self, x):\n","        # Get the number of samples in the batch\n","        batch_size = x.size(0)\n","        \n","        # Expand the input `x` to have dimensions: (batch_size, num_routes, num_capsules, in_channels, 1)\n","        # This makes it ready for matrix multiplication with the weights\n","        x = torch.stack([x] * self.num_capsules, dim=2).unsqueeze(4)\n","\n","        # Repeat the weight matrix `W` for each sample in the batch\n","        # Resulting dimensions: (batch_size, num_routes, num_capsules, out_channels, in_channels)\n","        W = torch.cat([self.W] * batch_size, dim=0)\n","        \n","        # Calculate the predicted output `u_hat` by multiplying weights `W` with inputs `x`\n","        # `u_hat` dimensions: (batch_size, num_routes, num_capsules, out_channels, 1)\n","        u_hat = torch.matmul(W, x)\n","\n","        # Initialize the 'logits' (initial guess for coupling coefficients) with zeros\n","        # `b_ij` dimensions: (1, num_routes, num_capsules, 1)\n","        b_ij = Variable(torch.zeros(1, self.num_routes, self.num_capsules, 1))\n","        if USE_CUDA:  # Move to GPU if available\n","            b_ij = b_ij.cuda()\n","\n","        num_iterations = 3  # Number of iterations for the routing algorithm\n","        for iteration in range(num_iterations):\n","            # Calculate the coupling coefficients using softmax along the capsule dimension\n","            # `c_ij` dimensions: (1, num_routes, num_capsules, 1)\n","            c_ij = F.softmax(b_ij, dim=2)\n","            # Repeat `c_ij` for each sample in the batch\n","            # `c_ij` dimensions: (batch_size, num_routes, num_capsules, 1, 1)\n","            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n","\n","            # Weighted sum of predictions (`u_hat`) to get the capsules' output\n","            # `s_j` dimensions: (batch_size, 1, num_capsules, out_channels, 1)\n","            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n","\n","            # Apply the squash function to limit the length of the output vector\n","            # `v_j` dimensions: (batch_size, 1, num_capsules, out_channels, 1)\n","            v_j = self.squash(s_j)\n","            \n","            if iteration < num_iterations - 1:\n","                # Calculate agreement (how well the predictions agree with the output)\n","                # `u_hat.transpose(3, 4)` swaps the last two dimensions of `u_hat`\n","                # `a_ij` dimensions: (batch_size, num_routes, num_capsules, 1, 1)\n","                a_ij = torch.matmul(u_hat.transpose(3, 4), torch.cat([v_j] * self.num_routes, dim=1))\n","\n","                # Update the logits (b_ij) based on the agreement\n","                # `a_ij.squeeze(4).mean(dim=0, keepdim=True)` reduces dimensions and calculates mean across the batch\n","                b_ij = b_ij + a_ij.squeeze(4).mean(dim=0, keepdim=True)\n","\n","        # Return the final output of the capsules, removing unnecessary dimensions\n","        return v_j.squeeze(1)\n","    \n","    def squash(self, input_tensor):\n","        # Squash function to ensure the output vectors have lengths between 0 and 1\n","        # Squared norm of the input tensor\n","        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n","        # Squash the input tensor using the squared norm\n","        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm + 1e-8))\n","        return output_tensor"]},{"cell_type":"markdown","metadata":{},"source":["**Reason:** This class combines the convolutional layer, primary capsules, and digit capsules into a full network. It includes methods for the forward pass, as well as loss calculation. The margin loss ensures that the network outputs high activations for the correct class and low activations for incorrect classes. The reconstruction loss is added as a regularizer, encouraging the capsules to encode useful information."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T05:30:00.703246Z","iopub.status.busy":"2024-07-07T05:30:00.702869Z","iopub.status.idle":"2024-07-07T05:30:00.714244Z","shell.execute_reply":"2024-07-07T05:30:00.713440Z","shell.execute_reply.started":"2024-07-07T05:30:00.703214Z"},"trusted":true},"outputs":[],"source":["# Define the Capsule Network (CapsNet)\n","class CapsNet(nn.Module):\n","    def __init__(self):\n","        super(CapsNet, self).__init__()\n","        \n","        # Initialize the different layers of the network\n","        self.conv_layer = ConvLayer()  # Convolutional layer to extract initial features from the input image\n","        self.primary_capsules = PrimaryCaps()  # Primary capsules layer to create the first level of capsules\n","        self.digit_capsules = DigitCaps()  # Digit capsules layer to create the final level of capsules\n","        self.decoder = Decoder()  # Decoder network to reconstruct the input image from the capsule output\n","        \n","        # Mean Squared Error loss function for reconstruction loss\n","        self.mse_loss = nn.MSELoss()\n","        \n","    def forward(self, data):\n","        # Pass the input data through the network layers\n","        output = self.digit_capsules(self.primary_capsules(self.conv_layer(data)))\n","        # Get the reconstructions and masked output from the decoder\n","        reconstructions, masked = self.decoder(output, data)\n","        return output, reconstructions, masked\n","    \n","    def loss(self, data, x, target, reconstructions):\n","        # Calculate the total loss as the sum of margin loss and reconstruction loss\n","        return self.margin_loss(x, target) + self.reconstruction_loss(data, reconstructions)\n","    \n","    def margin_loss(self, x, labels, size_average=True):\n","        batch_size = x.size(0)  # Get the batch size\n","\n","        # Calculate the length of the output vector from the digit capsules\n","        v_c = torch.sqrt((x**2).sum(dim=2, keepdim=True))\n","\n","        # Calculate the left part of the margin loss\n","        left = F.relu(0.9 - v_c).view(batch_size, -1)\n","        # Calculate the right part of the margin loss\n","        right = F.relu(v_c - 0.1).view(batch_size, -1)\n","\n","        # Combine the left and right parts to get the total margin loss\n","        # `labels` is used to weight the loss for each class\n","        loss = labels * left + 0.5 * (1.0 - labels) * right\n","        loss = loss.sum(dim=1).mean()  # Average the loss over the batch\n","\n","        return loss\n","    \n","    def reconstruction_loss(self, data, reconstructions):\n","        # Calculate the reconstruction loss using Mean Squared Error\n","        loss = self.mse_loss(reconstructions.view(reconstructions.size(0), -1), data.view(reconstructions.size(0), -1))\n","        return loss * 0.0005  # Scale the reconstruction loss to balance with the margin loss\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T05:30:00.758334Z","iopub.status.busy":"2024-07-07T05:30:00.758026Z","iopub.status.idle":"2024-07-07T05:30:00.766734Z","shell.execute_reply":"2024-07-07T05:30:00.765818Z","shell.execute_reply.started":"2024-07-07T05:30:00.758306Z"},"trusted":true},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self):\n","        super(Decoder, self).__init__()\n","        \n","        # Define the reconstruction layers of the Decoder using a Sequential container\n","        self.reconstruction_layers = nn.Sequential(\n","            nn.Linear(16 * 10, 512),  # First layer: Linear transformation from 16*10 dimensions to 512\n","            nn.ReLU(inplace=True),  # ReLU activation function\n","            nn.Linear(512, 1024),  # Second layer: Linear transformation from 512 to 1024 dimensions\n","            nn.ReLU(inplace=True),  # ReLU activation function\n","            nn.Linear(1024, 784),  # Third layer: Linear transformation from 1024 to 784 dimensions\n","            nn.Sigmoid()  # Sigmoid activation function to get output values between 0 and 1\n","        )\n","    \n","    def forward(self, x, data):\n","        # Calculate the length of the output vectors from the digit capsules\n","        classes = torch.sqrt((x ** 2).sum(2))\n","        # Apply softmax to get probabilities for each class\n","        classes = F.softmax(classes, dim=1)\n","        \n","        # Get the index of the maximum value for each sample in the batch\n","        _, max_length_indices = classes.max(dim=1)\n","        \n","        # Create a one-hot encoding for the classes with the maximum value\n","        masked = Variable(torch.sparse.torch.eye(10))\n","        if USE_CUDA:  # Move to GPU if available\n","            masked = masked.cuda()\n","        \n","        # Select the rows corresponding to the max_length_indices\n","        masked = masked.index_select(dim=0, index=max_length_indices.squeeze(1).data)\n","        \n","        # Multiply the output of the digit capsules with the masked tensor to keep only the values corresponding to the predicted class\n","        reconstructions = self.reconstruction_layers((x * masked[:, :, None, None]).view(x.size(0), -1))\n","        # Reshape the reconstructions to the original image size (batch_size, 1, 28, 28)\n","        reconstructions = reconstructions.view(-1, 1, 28, 28)\n","        \n","        return reconstructions, masked\n"]},{"cell_type":"markdown","metadata":{},"source":["**Reason: ** This class defines the combined loss function for training the Capsule Network. It includes both the margin loss and the reconstruction loss, balancing them with a scaling factor"]},{"cell_type":"markdown","metadata":{},"source":["## Loading the Dataset "]},{"cell_type":"markdown","metadata":{},"source":["We are using MINIST Dataset becuase author ussage the same dataset."]},{"cell_type":"markdown","metadata":{},"source":["**Reason:** The MNIST dataset is loaded using torchvision.datasets, and appropriate transformations (grayscale, normalization) are applied. DataLoader is used to create batches and shuffle the data during training, improving the training process."]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T05:30:00.769251Z","iopub.status.busy":"2024-07-07T05:30:00.768627Z","iopub.status.idle":"2024-07-07T05:30:00.777707Z","shell.execute_reply":"2024-07-07T05:30:00.776940Z","shell.execute_reply.started":"2024-07-07T05:30:00.769219Z"},"trusted":true},"outputs":[],"source":["class Mnist:\n","    def __init__(self, batch_size):\n","        dataset_transform = transforms.Compose([\n","                       transforms.ToTensor(),\n","                       transforms.Normalize((0.1307,), (0.3081,))\n","                   ])\n","\n","        train_dataset = datasets.MNIST('../data', train=True, download=True, transform=dataset_transform)\n","        test_dataset = datasets.MNIST('../data', train=False, download=True, transform=dataset_transform)\n","        \n","        self.train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","        self.test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Training The Model and Evaluation"]},{"cell_type":"markdown","metadata":{},"source":["### Pytorch Training Loop"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T05:30:00.779639Z","iopub.status.busy":"2024-07-07T05:30:00.778956Z","iopub.status.idle":"2024-07-07T05:30:00.878186Z","shell.execute_reply":"2024-07-07T05:30:00.877227Z","shell.execute_reply.started":"2024-07-07T05:30:00.779608Z"},"trusted":true},"outputs":[],"source":["capsule_net = CapsNet()\n","if USE_CUDA:\n","    capsule_net = capsule_net.cuda()\n","optimizer = Adam(capsule_net.parameters())"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T05:31:05.962896Z","iopub.status.busy":"2024-07-07T05:31:05.962126Z","iopub.status.idle":"2024-07-07T05:31:06.039567Z","shell.execute_reply":"2024-07-07T05:31:06.038575Z","shell.execute_reply.started":"2024-07-07T05:31:05.962862Z"},"trusted":true},"outputs":[],"source":["batch_size = 100\n","mnist = Mnist(batch_size)\n","n_epochs = 3"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T05:31:08.814348Z","iopub.status.busy":"2024-07-07T05:31:08.813429Z","iopub.status.idle":"2024-07-07T05:42:04.637910Z","shell.execute_reply":"2024-07-07T05:42:04.636613Z","shell.execute_reply.started":"2024-07-07T05:31:08.814301Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_319/1343296309.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  c_ij = F.softmax(b_ij)\n","/tmp/ipykernel_319/2259784343.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  classes = F.softmax(classes)\n"]},{"name":"stdout","output_type":"stream","text":["train accuracy: 0.84\n","train accuracy: 0.95\n","train accuracy: 0.96\n","train accuracy: 0.99\n","train accuracy: 0.99\n","train accuracy: 0.99\n","0.174850958827883\n","test accuracy: 0.97\n","0.049070835188031195\n","train accuracy: 0.98\n","train accuracy: 0.97\n","train accuracy: 0.97\n","train accuracy: 0.97\n","train accuracy: 0.99\n","train accuracy: 0.98\n","0.043808371781681975\n","test accuracy: 0.98\n","0.034780456111766396\n","train accuracy: 0.99\n","train accuracy: 1.0\n","train accuracy: 0.98\n","train accuracy: 0.99\n","train accuracy: 0.99\n","train accuracy: 0.99\n","0.032574088972760366\n","test accuracy: 0.99\n","0.02911032467149198\n","train accuracy: 1.0\n","train accuracy: 0.99\n","train accuracy: 0.99\n","train accuracy: 1.0\n","train accuracy: 0.98\n","train accuracy: 0.99\n","0.027303031081488977\n","test accuracy: 0.98\n","0.026905517932027577\n","train accuracy: 0.98\n","train accuracy: 1.0\n","train accuracy: 0.99\n","train accuracy: 1.0\n","train accuracy: 1.0\n","train accuracy: 0.1\n","nan\n","test accuracy: 0.05\n","nan\n"]}],"source":["for epoch in range(n_epochs):\n","    capsule_net.train()  # Set the network to training mode\n","    train_loss = 0  # Initialize training loss for the epoch\n","    \n","    for batch_id, (data, target) in enumerate(mnist.train_loader):\n","        # Convert target labels to one-hot encoding\n","        target = torch.sparse.torch.eye(10).index_select(dim=0, index=target)\n","        # Wrap data and target in Variables for autograd\n","        data, target = Variable(data), Variable(target)\n","\n","        # Move data and target to GPU if available\n","        if USE_CUDA:\n","            data, target = data.cuda(), target.cuda()\n","\n","        optimizer.zero_grad()  # Zero the gradients of the optimizer\n","        output, reconstructions, masked = capsule_net(data)  # Forward pass\n","        loss = capsule_net.loss(data, output, target, reconstructions)  # Calculate loss\n","        loss.backward()  # Backward pass (compute gradients)\n","        optimizer.step()  # Update weights\n","\n","        train_loss += loss.item()  # Accumulate training loss\n","        \n","        if batch_id % 100 == 0:  # Print accuracy every 100 batches\n","            accuracy = sum(np.argmax(masked.data.cpu().numpy(), 1) ==\n","                           np.argmax(target.data.cpu().numpy(), 1)) / float(data.size(0))\n","            print(\"train accuracy:\", accuracy)\n","    \n","    # Print average training loss for the epoch\n","    print(\"Epoch:\", epoch, \"Training Loss:\", train_loss / len(mnist.train_loader))\n","    \n","    # Evaluation loop\n","    capsule_net.eval()  # Set the network to evaluation mode\n","    test_loss = 0  # Initialize test loss\n","    \n","    for batch_id, (data, target) in enumerate(mnist.test_loader):\n","        # Convert target labels to one-hot encoding\n","        target = torch.sparse.torch.eye(10).index_select(dim=0, index=target)\n","        # Wrap data and target in Variables for autograd\n","        data, target = Variable(data), Variable(target)\n","\n","        # Move data and target to GPU if available\n","        if USE_CUDA:\n","            data, target = data.cuda(), target.cuda()\n","\n","        # Forward pass\n","        output, reconstructions, masked = capsule_net(data)\n","        # Calculate loss\n","        loss = capsule_net.loss(data, output, target, reconstructions)\n","        test_loss += loss.item()  # Accumulate test loss\n","        \n","        if batch_id % 100 == 0:  # Print accuracy every 100 batches\n","            accuracy = sum(np.argmax(masked.data.cpu().numpy(), 1) ==\n","                           np.argmax(target.data.cpu().numpy(), 1)) / float(data.size(0))\n","            print(\"test accuracy:\", accuracy)\n","    \n","    # Print average test loss for the epoch\n","    print(\"Epoch:\", epoch, \"Test Loss:\", test_loss / len(mnist.test_loader))\n"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T05:42:04.640469Z","iopub.status.busy":"2024-07-07T05:42:04.640039Z","iopub.status.idle":"2024-07-07T05:42:04.646868Z","shell.execute_reply":"2024-07-07T05:42:04.645942Z","shell.execute_reply.started":"2024-07-07T05:42:04.640414Z"},"trusted":true},"outputs":[],"source":["import matplotlib\n","import matplotlib.pyplot as plt\n","\n","def plot_images_separately(images):\n","    \"Plot the six MNIST images separately.\"\n","    fig = plt.figure()\n","    for j in range(1, 7):\n","        ax = fig.add_subplot(1, 6, j)\n","        ax.matshow(images[j-1], cmap = matplotlib.cm.binary)\n","        plt.xticks(np.array([]))\n","        plt.yticks(np.array([]))\n","    plt.show()\n","\n"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T05:42:52.860485Z","iopub.status.busy":"2024-07-07T05:42:52.859858Z","iopub.status.idle":"2024-07-07T05:42:53.184799Z","shell.execute_reply":"2024-07-07T05:42:53.183378Z","shell.execute_reply.started":"2024-07-07T05:42:52.860441Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 6 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_images_separately(data[:6,0].data.cpu().numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":102285,"sourceId":242592,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
