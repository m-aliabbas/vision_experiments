{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":242592,"sourceType":"datasetVersion","datasetId":102285}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CapsNet MNIST Experiment","metadata":{}},{"cell_type":"markdown","source":"## Envoirnment Setting ","metadata":{}},{"cell_type":"markdown","source":"Import the Require Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.optim import Adam\nfrom torchvision import datasets, transforms\n\nUSE_CUDA = True","metadata":{"execution":{"iopub.status.busy":"2024-07-07T05:30:00.660083Z","iopub.execute_input":"2024-07-07T05:30:00.660555Z","iopub.status.idle":"2024-07-07T05:30:00.666421Z","shell.execute_reply.started":"2024-07-07T05:30:00.660501Z","shell.execute_reply":"2024-07-07T05:30:00.665499Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## The Architecture of CapsNet","metadata":{}},{"cell_type":"markdown","source":"A block of Convolution layer and Relu Activation on it","metadata":{}},{"cell_type":"code","source":"class ConvLayer(nn.Module):\n    def __init__(self, in_channels=1, out_channels=256, kernel_size=9):\n        super(ConvLayer, self).__init__()\n\n        self.conv = nn.Conv2d(in_channels=in_channels,\n                               out_channels=out_channels,\n                               kernel_size=kernel_size,\n                               stride=1\n                             )\n\n    def forward(self, x):\n        return F.relu(self.conv(x))","metadata":{"execution":{"iopub.status.busy":"2024-07-07T05:30:00.668434Z","iopub.execute_input":"2024-07-07T05:30:00.668747Z","iopub.status.idle":"2024-07-07T05:30:00.675922Z","shell.execute_reply.started":"2024-07-07T05:30:00.668722Z","shell.execute_reply":"2024-07-07T05:30:00.675122Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"**Reason:** This class creates the first layer of capsules directly from the input data. It transforms the convolutional output into capsule vectors, preparing them for the next layers.","metadata":{}},{"cell_type":"code","source":"class PrimaryCaps(nn.Module):\n    def __init__(self, num_capsules=8, in_channels=256, out_channels=32, kernel_size=9):\n        super(PrimaryCaps, self).__init__()\n\n        self.capsules = nn.ModuleList([\n            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=2, padding=0) \n                          for _ in range(num_capsules)])\n    \n    def forward(self, x):\n        u = [capsule(x) for capsule in self.capsules]\n        u = torch.stack(u, dim=1)\n        u = u.view(x.size(0), 32 * 6 * 6, -1)\n        return self.squash(u)\n    \n    def squash(self, input_tensor):\n        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n        return output_tensor","metadata":{"execution":{"iopub.status.busy":"2024-07-07T05:30:00.676893Z","iopub.execute_input":"2024-07-07T05:30:00.677230Z","iopub.status.idle":"2024-07-07T05:30:00.687857Z","shell.execute_reply.started":"2024-07-07T05:30:00.677206Z","shell.execute_reply":"2024-07-07T05:30:00.686878Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"**Reason:** This class represents the digit capsules, responsible for high-level feature detection. It performs the dynamic routing-by-agreement algorithm, which iteratively updates the coupling coefficients (weights) to determine the contribution of each capsule from the previous layer to each capsule in the current layer.","metadata":{}},{"cell_type":"code","source":"class DigitCaps(nn.Module):\n    def __init__(self, num_capsules=10, num_routes=32 * 6 * 6, in_channels=8, out_channels=16):\n        super(DigitCaps, self).__init__()\n\n        self.in_channels = in_channels\n        self.num_routes = num_routes\n        self.num_capsules = num_capsules\n\n        self.W = nn.Parameter(torch.randn(1, num_routes, num_capsules, out_channels, in_channels))\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        x = torch.stack([x] * self.num_capsules, dim=2).unsqueeze(4)\n\n        W = torch.cat([self.W] * batch_size, dim=0)\n        u_hat = torch.matmul(W, x)\n\n        b_ij = Variable(torch.zeros(1, self.num_routes, self.num_capsules, 1))\n        if USE_CUDA:\n            b_ij = b_ij.cuda()\n\n        num_iterations = 3\n        for iteration in range(num_iterations):\n            c_ij = F.softmax(b_ij)\n            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n\n            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n            v_j = self.squash(s_j)\n            \n            if iteration < num_iterations - 1:\n                a_ij = torch.matmul(u_hat.transpose(3, 4), torch.cat([v_j] * self.num_routes, dim=1))\n                b_ij = b_ij + a_ij.squeeze(4).mean(dim=0, keepdim=True)\n\n        return v_j.squeeze(1)\n    \n    def squash(self, input_tensor):\n        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n        return output_tensor\n","metadata":{"execution":{"iopub.status.busy":"2024-07-07T05:30:00.688895Z","iopub.execute_input":"2024-07-07T05:30:00.689151Z","iopub.status.idle":"2024-07-07T05:30:00.700511Z","shell.execute_reply.started":"2024-07-07T05:30:00.689129Z","shell.execute_reply":"2024-07-07T05:30:00.699629Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"**Reason:** This class combines the convolutional layer, primary capsules, and digit capsules into a full network. It includes methods for the forward pass, as well as loss calculation. The margin loss ensures that the network outputs high activations for the correct class and low activations for incorrect classes. The reconstruction loss is added as a regularizer, encouraging the capsules to encode useful information.","metadata":{}},{"cell_type":"code","source":"class CapsNet(nn.Module):\n    def __init__(self):\n        super(CapsNet, self).__init__()\n        self.conv_layer = ConvLayer()\n        self.primary_capsules = PrimaryCaps()\n        self.digit_capsules = DigitCaps()\n        self.decoder = Decoder()\n        \n        self.mse_loss = nn.MSELoss()\n        \n    def forward(self, data):\n        output = self.digit_capsules(self.primary_capsules(self.conv_layer(data)))\n        reconstructions, masked = self.decoder(output, data)\n        return output, reconstructions, masked\n    \n    def loss(self, data, x, target, reconstructions):\n        return self.margin_loss(x, target) + self.reconstruction_loss(data, reconstructions)\n    \n    def margin_loss(self, x, labels, size_average=True):\n        batch_size = x.size(0)\n\n        v_c = torch.sqrt((x**2).sum(dim=2, keepdim=True))\n\n        left = F.relu(0.9 - v_c).view(batch_size, -1)\n        right = F.relu(v_c - 0.1).view(batch_size, -1)\n\n        loss = labels * left + 0.5 * (1.0 - labels) * right\n        loss = loss.sum(dim=1).mean()\n\n        return loss\n    \n    def reconstruction_loss(self, data, reconstructions):\n        loss = self.mse_loss(reconstructions.view(reconstructions.size(0), -1), data.view(reconstructions.size(0), -1))\n        return loss * 0.0005\n","metadata":{"execution":{"iopub.status.busy":"2024-07-07T05:30:00.702869Z","iopub.execute_input":"2024-07-07T05:30:00.703246Z","iopub.status.idle":"2024-07-07T05:30:00.714244Z","shell.execute_reply.started":"2024-07-07T05:30:00.703214Z","shell.execute_reply":"2024-07-07T05:30:00.713440Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self):\n        super(Decoder, self).__init__()\n        \n        self.reconstraction_layers = nn.Sequential(\n            nn.Linear(16 * 10, 512),\n            nn.ReLU(inplace=True),\n            nn.Linear(512, 1024),\n            nn.ReLU(inplace=True),\n            nn.Linear(1024, 784),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x, data):\n        classes = torch.sqrt((x ** 2).sum(2))\n        classes = F.softmax(classes)\n        \n        _, max_length_indices = classes.max(dim=1)\n        masked = Variable(torch.sparse.torch.eye(10))\n        if USE_CUDA:\n            masked = masked.cuda()\n        masked = masked.index_select(dim=0, index=max_length_indices.squeeze(1).data)\n        \n        reconstructions = self.reconstraction_layers((x * masked[:, :, None, None]).view(x.size(0), -1))\n        reconstructions = reconstructions.view(-1, 1, 28, 28)\n        \n        return reconstructions, masked\n","metadata":{"execution":{"iopub.status.busy":"2024-07-07T05:30:00.758026Z","iopub.execute_input":"2024-07-07T05:30:00.758334Z","iopub.status.idle":"2024-07-07T05:30:00.766734Z","shell.execute_reply.started":"2024-07-07T05:30:00.758306Z","shell.execute_reply":"2024-07-07T05:30:00.765818Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"**Reason: ** This class defines the combined loss function for training the Capsule Network. It includes both the margin loss and the reconstruction loss, balancing them with a scaling factor","metadata":{}},{"cell_type":"markdown","source":"## Loading the Dataset ","metadata":{}},{"cell_type":"markdown","source":"We are using MINIST Dataset becuase author ussage the same dataset.","metadata":{}},{"cell_type":"markdown","source":"**Reason:** The MNIST dataset is loaded using torchvision.datasets, and appropriate transformations (grayscale, normalization) are applied. DataLoader is used to create batches and shuffle the data during training, improving the training process.","metadata":{}},{"cell_type":"code","source":"class Mnist:\n    def __init__(self, batch_size):\n        dataset_transform = transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])\n\n        train_dataset = datasets.MNIST('../data', train=True, download=True, transform=dataset_transform)\n        test_dataset = datasets.MNIST('../data', train=False, download=True, transform=dataset_transform)\n        \n        self.train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n        self.test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T05:30:00.768627Z","iopub.execute_input":"2024-07-07T05:30:00.769251Z","iopub.status.idle":"2024-07-07T05:30:00.777707Z","shell.execute_reply.started":"2024-07-07T05:30:00.769219Z","shell.execute_reply":"2024-07-07T05:30:00.776940Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## Training The Model and Evaluation","metadata":{}},{"cell_type":"markdown","source":"### Pytorch Training Loop","metadata":{}},{"cell_type":"code","source":"capsule_net = CapsNet()\nif USE_CUDA:\n    capsule_net = capsule_net.cuda()\noptimizer = Adam(capsule_net.parameters())","metadata":{"execution":{"iopub.status.busy":"2024-07-07T05:30:00.778956Z","iopub.execute_input":"2024-07-07T05:30:00.779639Z","iopub.status.idle":"2024-07-07T05:30:00.878186Z","shell.execute_reply.started":"2024-07-07T05:30:00.779608Z","shell.execute_reply":"2024-07-07T05:30:00.877227Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"batch_size = 100\nmnist = Mnist(batch_size)\nn_epochs = 3","metadata":{"execution":{"iopub.status.busy":"2024-07-07T05:31:05.962126Z","iopub.execute_input":"2024-07-07T05:31:05.962896Z","iopub.status.idle":"2024-07-07T05:31:06.039567Z","shell.execute_reply.started":"2024-07-07T05:31:05.962862Z","shell.execute_reply":"2024-07-07T05:31:06.038575Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"\nfor epoch in range(n_epochs):\n    capsule_net.train()\n    train_loss = 0\n    for batch_id, (data, target) in enumerate(mnist.train_loader):\n\n        target = torch.sparse.torch.eye(10).index_select(dim=0, index=target)\n        data, target = Variable(data), Variable(target)\n\n        if USE_CUDA:\n            data, target = data.cuda(), target.cuda()\n\n        optimizer.zero_grad()\n        output, reconstructions, masked = capsule_net(data)\n        loss = capsule_net.loss(data, output, target, reconstructions)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        \n        if batch_id % 100 == 0:\n            print(\"train accuracy:\", sum(np.argmax(masked.data.cpu().numpy(), 1) == \n                                   np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size))\n        \n    print(train_loss / len(mnist.train_loader))\n        \n    capsule_net.eval()\n    test_loss = 0\n    for batch_id, (data, target) in enumerate(mnist.test_loader):\n\n        target = torch.sparse.torch.eye(10).index_select(dim=0, index=target)\n        data, target = Variable(data), Variable(target)\n\n        if USE_CUDA:\n            data, target = data.cuda(), target.cuda()\n\n        output, reconstructions, masked = capsule_net(data)\n        loss = capsule_net.loss(data, output, target, reconstructions)\n\n        test_loss += loss.item()\n        \n        if batch_id % 100 == 0:\n            print(\"test accuracy:\", sum(np.argmax(masked.data.cpu().numpy(), 1) == \n                                   np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size))\n    \n    print(test_loss / len(mnist.test_loader))\n","metadata":{"execution":{"iopub.status.busy":"2024-07-07T05:31:08.813429Z","iopub.execute_input":"2024-07-07T05:31:08.814348Z","iopub.status.idle":"2024-07-07T05:42:04.637910Z","shell.execute_reply.started":"2024-07-07T05:31:08.814301Z","shell.execute_reply":"2024-07-07T05:42:04.636613Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_319/1343296309.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  c_ij = F.softmax(b_ij)\n/tmp/ipykernel_319/2259784343.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  classes = F.softmax(classes)\n","output_type":"stream"},{"name":"stdout","text":"train accuracy: 0.84\ntrain accuracy: 0.95\ntrain accuracy: 0.96\ntrain accuracy: 0.99\ntrain accuracy: 0.99\ntrain accuracy: 0.99\n0.174850958827883\ntest accuracy: 0.97\n0.049070835188031195\ntrain accuracy: 0.98\ntrain accuracy: 0.97\ntrain accuracy: 0.97\ntrain accuracy: 0.97\ntrain accuracy: 0.99\ntrain accuracy: 0.98\n0.043808371781681975\ntest accuracy: 0.98\n0.034780456111766396\ntrain accuracy: 0.99\ntrain accuracy: 1.0\ntrain accuracy: 0.98\ntrain accuracy: 0.99\ntrain accuracy: 0.99\ntrain accuracy: 0.99\n0.032574088972760366\ntest accuracy: 0.99\n0.02911032467149198\ntrain accuracy: 1.0\ntrain accuracy: 0.99\ntrain accuracy: 0.99\ntrain accuracy: 1.0\ntrain accuracy: 0.98\ntrain accuracy: 0.99\n0.027303031081488977\ntest accuracy: 0.98\n0.026905517932027577\ntrain accuracy: 0.98\ntrain accuracy: 1.0\ntrain accuracy: 0.99\ntrain accuracy: 1.0\ntrain accuracy: 1.0\ntrain accuracy: 0.1\nnan\ntest accuracy: 0.05\nnan\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib\nimport matplotlib.pyplot as plt\n\ndef plot_images_separately(images):\n    \"Plot the six MNIST images separately.\"\n    fig = plt.figure()\n    for j in range(1, 7):\n        ax = fig.add_subplot(1, 6, j)\n        ax.matshow(images[j-1], cmap = matplotlib.cm.binary)\n        plt.xticks(np.array([]))\n        plt.yticks(np.array([]))\n    plt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-07T05:42:04.640039Z","iopub.execute_input":"2024-07-07T05:42:04.640469Z","iopub.status.idle":"2024-07-07T05:42:04.646868Z","shell.execute_reply.started":"2024-07-07T05:42:04.640414Z","shell.execute_reply":"2024-07-07T05:42:04.645942Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"plot_images_separately(data[:6,0].data.cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2024-07-07T05:42:52.859858Z","iopub.execute_input":"2024-07-07T05:42:52.860485Z","iopub.status.idle":"2024-07-07T05:42:53.184799Z","shell.execute_reply.started":"2024-07-07T05:42:52.860441Z","shell.execute_reply":"2024-07-07T05:42:53.183378Z"},"trusted":true},"execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 6 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}